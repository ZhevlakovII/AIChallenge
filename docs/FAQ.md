# Часто задаваемые вопросы (FAQ)

## 1. Как работает чат с LLM?

Функция чата использует клиент-серверную архитектуру для взаимодействия с большими языковыми моделями (LLM). Когда вы отправляете сообщение:

1. Ваше сообщение отправляется на настроенный провайдер LLM (OpenAI, Anthropic, локальная модель и т.д.)
2. Приложение включает историю разговора в качестве контекста
3. LLM обрабатывает ваш запрос и генерирует ответ
4. Ответ передается обратно в интерфейс чата в режиме реального времени

Чат поддерживает несколько потоков разговора, историю сообщений и контекстно-зависимые ответы.

## 2. Какие настройки доступны для LLM?

Приложение предоставляет следующие настраиваемые параметры для провайдеров LLM:

- **API_KEY**: Ключ аутентификации для доступа к API провайдера LLM
- **baseUrl**: URL-адрес конечной точки для сервиса LLM (например, `https://api.openai.com/v1`)
- **model**: Конкретная модель для использования (например, `gpt-4`, `claude-3-opus`, `llama-3.1-8b`)
- **topK**: Управляет разнообразием ответов, ограничивая количество рассматриваемых токенов с наивысшей вероятностью (по умолчанию: 40)

## 3. Откуда берутся значения этих настроек?

Все настройки конфигурации LLM хранятся локально на вашем устройстве:

- **Android**: Приватное хранилище приложения с использованием DataStore Preferences
- **Desktop (JVM)**: Локальные конфигурационные файлы в директории данных приложения
- **Доступ к настройкам**: Перейдите в Настройки → Конфигурация LLM для просмотра и изменения этих значений

Ваши API-ключи и учетные данные никогда не отправляются третьим лицам, кроме настроенного провайдера LLM.

## 4. Что делать, если модель не отвечает?

Если вы не получаете ответы от LLM, проверьте следующее:

**Проблемы с сетью:**
- Убедитесь, что ваше интернет-соединение стабильно
- Проверьте, доступен ли baseUrl (попробуйте открыть его в браузере)
- Убедитесь, что ваш брандмауэр не блокирует соединение

**Проблемы с конфигурацией:**
- Проверьте, что ваш API_KEY действителен и не истек
- Подтвердите, что baseUrl правильный для вашего провайдера
- Убедитесь, что выбранная модель доступна с вашим API-ключом

**Проблемы с таймаутом:**
- Некоторые модели могут дольше отвечать на сложные запросы
- Попробуйте сначала с более коротким, простым запросом
- Рассмотрите возможность увеличения таймаута в расширенных настройках

## 5. Типичная проблема: Неправильный API-ключ

**Симптомы:**
- Ошибка "Аутентификация не удалась" или "401 Unauthorized"
- Сообщение "Недействительный API-ключ"

**Решения:**
1. Тщательно проверьте ваш API_KEY на опечатки (они часто содержат специальные символы)
2. Убедитесь, что ключ не истек в панели управления вашего провайдера
3. Убедитесь, что вы используете правильный ключ для выбранного провайдера
4. Попробуйте сгенерировать новый API-ключ на сайте вашего провайдера

## 6. Типичная проблема: Неверный URL

**Симптомы:**
- Ошибка "Connection refused" или "404 Not Found"
- Сообщение "Невозможно достичь сервера"

**Решения:**
1. Проверьте, что формат baseUrl включает протокол (https://)
2. Распространенные URL:
   - OpenAI: `https://api.openai.com/v1`
   - Anthropic: `https://api.anthropic.com/v1`
   - Локальный Ollama: `http://localhost:11434`
3. Удалите завершающие слэши из URL
4. Проверьте документацию провайдера на предмет правильной конечной точки

## 7. Типичная проблема: Модель не найдена

**Симптомы:**
- Ошибка "Model not found" или "404"
- Сообщение "Модель `xyz` не существует"

**Решения:**
1. Убедитесь, что имя модели написано правильно (с учетом регистра)
2. Проверьте, имеет ли ваш API-ключ доступ к этой модели
3. Посетите документацию вашего провайдера для доступных моделей
4. Некоторые модели требуют специального доступа или подписки более высокого уровня
5. Попробуйте использовать другую, широко доступную модель:
   - OpenAI: `gpt-3.5-turbo`, `gpt-4`
   - Anthropic: `claude-3-sonnet-20240229`

## 8. Производительность и оптимизация

### Медленный отклик чата

**Симптомы:**
- Ответы от LLM занимают 30-40 секунд даже на простые вопросы
- Задержки при использовании локальных моделей (Ollama)
- Чат зависает на длинных сообщениях

**Причины:**
- Сетевые проблемы или медленное интернет-соединение
- Высокая нагрузка на API провайдера
- Недостаточные ресурсы для локальных моделей
- Ограничения производительности устройства

**Решения:**
1. **Проверьте сетевое соединение:**
   - Убедитесь в стабильности интернет-подключения
   - При медленном интернете попробуйте более простые модели

2. **Оптимизация для локальных моделей:**
   - Используйте более легкие модели (llama-3.1-8b вместо 70b)
   - Убедитесь, что на устройстве достаточно RAM (рекомендуется 16GB+ для 8B моделей)
   - Проверьте загрузку CPU - другие процессы могут тормозить модель

3. **Для облачных API:**
   - Попробуйте другое время дня (меньшая нагрузка на серверы)
   - Рассмотрите использование более быстрых моделей (gpt-3.5-turbo вместо gpt-4)

### Зависания на длинных сообщениях

**Симптомы:**
- Приложение полностью зависает при отправке сообщений длиннее 2000 символов
- Не реагирует на действия пользовательского интерфейса
- Требуется принудительное перезапуск приложения

**Причины:**
- Проблемы с обработкой больших токенов
- Ограничения памяти при обработке длинных контекстов
- Баги в обработке потоковых ответов

**Решения:**
1. **Разделите длинные сообщения:**
   - Разбивайте сложные запросы на несколько частей
   - Используйте короткие, конкретные вопросы

2. **Очистка контекста:**
   - Периодически начинайте новый чат для сброса контекста
   - Удаляйте ненужные сообщения из истории

3. **Альтернативные подходы:**
   - Используйте файловый обмен для очень длинных текстов
   - Рассмотрите использование API напрямую для больших объемов

---

## 9. Ошибки и диагностика

### Проблемы после обновления приложения

**Симптомы:**
- LLM перестал отвечать после обновления до последней версии
- В логах появляется "Internal Server Error 500"
- Ранее работающие настройки больше не функционируют

**Причины:**
- Изменения в формате конфигурации
- Совместимость с новыми версиями зависимостей
- Обновления в API провайдеров

**Решения:**
1. **Проверьте настройки конфигурации:**
   - Убедитесь, что все параметры API_KEY, baseUrl, model на месте
   - Проверьте, что формат URL соответствует новым требованиям

2. **Очистка кэша:**
   - Очистите кэш приложения
   - Перезапустите приложение после очистки

3. **Переустановка:**
   - Если проблемы продолжаются, выполните полную переустановку
   - Сохраните настройки перед переустановкой

### Внутренние ошибки сервера (500)

**Симптомы:**
- Появление ошибки "Internal Server Error 500" в логах
- Отсутствие ответов от LLM при корректных настройках
- Нестабильная работа приложения

**Причины:**
- Проблемы на стороне API провайдера
- Несовместимость версий API
- Проблемы с сетевым подключением

**Решения:**
1. **Диагностика проблемы:**
   - Проверьте статус API провайдера (status.openai.com, status.anthropic.com)
   - Попробуйте тот же запрос через API клиент

2. **Временные решения:**
   - Используйте альтернативного провайдера
   - Попробуйте другую модель

3. **Постоянные решения:**
   - Обновитесь до последней версии приложения
   - Свяжитесь с поддержкой при стабильных ошибках

---

## 10. Работа с параметрами

### Параметр topK не работает

**Симптомы:**
- Изменение значения topK не влияет на разнообразие ответов
- Ответы остаются такими же креативными/детерминированными
- Параметр кажется проигнорированным

**Причины:**
- OpenAI API не поддерживает параметр topK
- Разные провайдеры используют разные параметры контроля разнообразия

**Решения:**
1. **Используйте правильные параметры для OpenAI:**
   - `temperature`: Контроль креативности (0.0-2.0)
   - `top_p`: Альтернатива topK (0.0-1.0)

2. **Рекомендуемые настройки:**
   - Для детерминированных ответов: temperature=0.0, top_p=1.0
   - Для креативных ответов: temperature=0.7-1.0, top_p=0.9
   - Для баланса: temperature=0.3-0.5, top_p=0.95

3. **Провайдеры поддерживающие topK:**
   - Lokal модели (Ollama)
   - Некоторые open-source решения

### Валидация настроек

**Симптомы:**
- Ошибки валидации при сохранении настроек
- Непонятные сообщения об ошибках
- Невозможность сохранить корректные параметры

**Причины:**
- Неправильный формат URL (отсутствие https://)
- Недопустимые символы в параметрах
- Ограничения значений параметров

**Решения:**
1. **Правильный формат baseUrl:**
   - Всегда включайте протокол: `https://api.openai.com/v1`
   - Удаляйте завершающие слэши
   - Проверьте точность URL для вашего провайдера

2. **Диапазоны значений параметров:**
   - temperature: 0.0 - 2.0
   - top_p: 0.0 - 1.0
   - topK: 1 - 100 (где поддерживается)

3. **Проверка API ключей:**
   - Убедитесь в отсутствии пробелов в начале/конце
   - Проверьте актуальность ключа
   - Используйте свежий ключ при сомнениях

---

## 11. Product Assistant - Управление тикетами поддержки

### Как работает Product Assistant?

Product Assistant - это интеллектуальный помощник, который объединяет три основных функции:

1. **FAQ и база знаний** - Поиск ответов в документации проекта через RAG
2. **Анализ тикетов** - Работа с тикетами поддержки через MCP инструменты
3. **Генерация ответов** - Создание контекстных ответов с помощью LLM

### Режимы работы Product Assistant

**Режим FAQ:**
- Использует только базу знаний и документацию
- Идеален для вопросов о функциях продукта
- Быстрые ответы на основе существующей документации

**Анализ тикетов:**
- Работает только с тикетами поддержки
- Анализирует похожие проблемы и решения
- Помогает в диагностике и устранении неисправностей

**Полный режим:**
- Комбинирует FAQ и анализ тикетов
- Provides наиболее полные и точные ответы
- Рекомендуется для сложных запросов

### Создание и управление тикетами

**Создание нового тикета:**
1. Нажмите кнопку "Создать тикет" в интерфейсе Product Assistant
2. Заполните заголовок (краткое описание проблемы)
3. Добавьте подробное описание проблемы
4. Укажите релевантные теги (по желанию)
5. Нажмите "Создать" для отправки

**Обновление тикета:**
- Изменение статуса (open → in_progress → resolved → closed)
- Добавление комментариев с дополнительной информацией
- Просмотр истории изменений

**Статусы тикетов:**
- **Open** - Новый тикет, требует рассмотрения
- **In Progress** - В процессе работы
- **Resolved** - Решен, ожидает подтверждения
- **Closed** - Закрыт (решен или отменен)

### Интеграция с MCP (Model Context Protocol)

Product Assistant использует MCP для интеграции с системами управления тикетами:

**Доступные MCP инструменты:**
- `support.list_tickets` - Получение списка тикетов с фильтрацией
- `support.get_ticket` - Получение детальной информации о тикете
- `support.create_ticket` - Создание нового тикета
- `support.update_ticket` - Обновление статуса и добавление комментариев

**Преимущества MCP интеграции:**
- Стандартизированный доступ к разным системам поддержки
- Безопасная передача данных через вебсокеты
- Расширяемость для подключения новых провайдеров

### Поиск релевантной информации

**Как работает поиск:**
1. Анализирует ваш запрос на ключевые слова
2. Ищет совпадения в заголовках и описаниях тикетов
3. Проверяет релевантность тегов
4. Учитывает статус тикета (открытые имеют приоритет)
5. Возвращает наиболее релевантные результаты

**Советы для эффективного поиска:**
- Используйте конкретные ключевые слова
- Указывайте названия функций или компонентов
- Включайте сообщения об ошибках
- Используйте английские термины для технических проблем

### Типичные проблемы и решения

**Проблема: MCP сервер недоступен**
- **Симптомы:** Ошибка подключения к `ws://127.0.0.1:9001/mcp`
- **Решение:** Убедитесь, что MCP сервер support запущен и доступен
- **Проверка:** `curl -I http://127.0.0.1:9001/health`

**Проблема: Тикет не создается**
- **Симптомы:** Ошибка при попытке создать тикет
- **Причины:** Отсутствие обязательных полей, проблемы с MCP
- **Решение:** Проверьте заполнение заголовка и описания

**Проблема: Нет результатов поиска**
- **Симптомы:** Поиск возвращает пустые результаты
- **Решение:** Попробуйте другие ключевые слова или проверьте орфографию

### Лучшие практики использования

**Для получения лучших результатов:**

1. **Конкретные запросы:**
   - Плохо: "Ничего не работает"
   - Хорошо: "Ошибка авторизации при входе через Google"

2. **Используйте режимы правильно:**
   - FAQ режим: для вопросов о функциях
   - Анализ тикетов: для проблем и ошибок
   - Полный режим: для комплексных запросов

3. **Создавайте информативные тикеты:**
   - Четкий заголовок с сутью проблемы
   - Подробное описание с шагами воспроизведения
   - Релевантные теги для лучшей категоризации

4. **Проверяйте-confidence уровень:**
   - Высокая уверенность (>80%): ответ можно доверять
   - Средняя (50-80%):答可能正确，但建议验证
   - Низкая (<50%): ответ требует дополнительной проверки

---

## 12. Типичная проблема: Слишком большое значение topK

**Симптомы:**
- Низкое качество или бессвязные ответы
- Неожиданные или случайные выходные данные
- Ошибки "Недействительный параметр"

**Решения:**
1. Сбросьте topK до значения по умолчанию (40)
2. Рекомендуемые диапазоны:
   - Низкий (10-20): Более целенаправленные, детерминированные ответы
   - Средний (30-50): Сбалансированная креативность и связность
   - Высокий (60-100): Более разнообразные, но потенциально менее связные
3. Если вы видите ошибки "Недействительный параметр", ваш провайдер может не поддерживать topK
4. Не все модели поддерживают параметр topK - проверьте документацию вашего провайдера
